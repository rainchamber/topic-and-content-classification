{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "iJozf_gxBXEC",
        "outputId": "14b0e653-fa05-4157-e391-5ed5983fede4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndal.ipynb\\n\\nCreated on Mar 30 2023\\n\\n@author: Lukas\\n\\nThis notebook is meant as an introduction to discriminative active learning (DAL),\\nand contains implementations for computing the H-Divergence between datasets and \\nfor running DAL.\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "h-divergence.ipynb\n",
        "\n",
        "Created on Mar 30 2023\n",
        "\n",
        "@author: Lukas\n",
        "\n",
        "This notebook is meant as an introduction to discriminative active learning (DAL),\n",
        "and contains implementations for computing the H-Divergence between datasets.\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CfY-pyZqBXEJ"
      },
      "source": [
        "First, we recall the definition of the H-Divergence. See here (https://melissadell.atlassian.net/wiki/spaces/TCC/pages/2584412161) for more background information and links to domain adaptation.\n",
        "\n",
        "*Definition (H-Divergence):* Let $X$ be a domain (dataset), and let $D_S$ and $D_T$ be two distributions over $X$ (source and target). Let $H$ be a hypothesis class over $X$ (set of possible classifiers). Then we define the $H$-Divergence between $D_S$ and $D_T$ as\n",
        "\n",
        "$d_H(D_S, D_T) = \\sup_{h \\in H} \\left| \\mathbb{P}_{x \\sim D_S} \\left[ h(x) = 1 \\right] - \\mathbb{P}_{x \\sim D_T} \\left[ h(x) = 1 \\right]\\right|$\n",
        "\n",
        "We are interested in the distributions of the labeled and unlabed datasets, denoted by $L$ and $U$, respectively, so the $H$-Divergence becomes\n",
        "\n",
        "$d_H(D_S, D_T) = \\sup_{h \\in H} \\left| \\frac{1}{|L|} \\sum_{x \\in L} h(x) - \\frac{1}{|U|} \\sum_{x \\in U }h(x) \\right|$\n",
        "\n",
        "where $h(x)$ denotes the probability which the model $h$ assigns to the event $x \\in L$. In order to (approximately) attain the supremum, we train a binary MLP classifier with the above expression as the loss function, i.e. we want it to output very high $h(x)$ for $x \\in L$ and very low $h(x)$ for $x \\in U$. We follow the original DAL paper and apply the classifier $h$ to the embeddings $\\phi(x)$, and not to the original data $x$ itself. Here, $\\phi$ is the model we would ultimately like to train using active learning (e.g. BERT).\n",
        "\n",
        "Note that, by definition, $d_H(D_S, D_T) \\in [0, 1]$, where we want $d_H(D_S, D_T) \\approx 0$, which would indicate that the model (on average) cannot distinguish between $L$ and $U$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "emmWESI3BXEM"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1whF8KoBXEN"
      },
      "source": [
        "**Basic functionality for computing the H-Divergence between two datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tIFEcwz_BXEO"
      },
      "outputs": [],
      "source": [
        "# compute the H-divergence between the labeled data and the unlabeled data\n",
        "\n",
        "def compute_H_divergence(labeled, unlabeled, model):\n",
        "    \"\"\"\n",
        "    A function that computes the H-divergence between the labeled and unlabeled data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    labeled: numpy array\n",
        "        The labeled data.\n",
        "\n",
        "    unlabeled: numpy array\n",
        "        The unlabeled data.\n",
        "\n",
        "    model: torch model\n",
        "        The discriminative model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    H_divergence : float\n",
        "        The H-divergence between the labeled and unlabeled data.\n",
        "        Must be between 0 and 1.\n",
        "    \"\"\"\n",
        "    # convert the data to torch tensors\n",
        "    labeled = torch.from_numpy(labeled).float()\n",
        "    unlabeled = torch.from_numpy(unlabeled).float()\n",
        "\n",
        "    # for each element in the labeled data, compute the probabilities of the classes\n",
        "    p_L = model(labeled)\n",
        "    p_U = model(unlabeled)\n",
        "\n",
        "    # sum the probabilities of class 0 for each element in the labeled data and divide by the number of elements\n",
        "    p_L_0 = torch.sum(p_L[:, 0])\n",
        "    p_L_0 /= labeled.shape[0]\n",
        "\n",
        "    # sum the probabilities of class 0 for each element in the unlabeled data and divide by the number of elements\n",
        "    p_U_0 = torch.sum(p_U[:, 0])\n",
        "    p_U_0 /= unlabeled.shape[0]\n",
        "\n",
        "    # compute the H-divergence as the absolute difference between p_U_0 and p_L_0\n",
        "    H_divergence = torch.abs(p_U_0 - p_L_0)\n",
        "\n",
        "    # convert the H-divergence to a numpy float\n",
        "    return H_divergence.item()\n",
        "\n",
        "\n",
        "# train a discriminative model on the labeled data and unlabeled data\n",
        "\n",
        "def train_discriminative_model(labeled, unlabeled, input_shape):\n",
        "    \"\"\"\n",
        "    A function that trains and returns a discriminative model on the labeled and unlabeled data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    labeled : numpy.ndarray\n",
        "        The labeled data.\n",
        "\n",
        "    unlabeled : numpy.ndarray\n",
        "        The unlabeled data.\n",
        "\n",
        "    input_shape : int\n",
        "        The number of features in the dataset.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : tf.keras.Sequential\n",
        "        The trained discriminative model.\n",
        "    \"\"\"\n",
        "\n",
        "    # create the binary dataset:\n",
        "    y_L = np.zeros((labeled.shape[0], 1), dtype='int')\n",
        "    y_U = np.ones((unlabeled.shape[0], 1), dtype='int')\n",
        "    X_train = np.vstack((labeled, unlabeled))\n",
        "    Y_train = np.vstack((y_L, y_U))\n",
        "    X_train = torch.from_numpy(X_train).float()\n",
        "    Y_train = torch.from_numpy(Y_train).squeeze()\n",
        "\n",
        "    # build the model:\n",
        "    model = get_discriminative_model(input_shape)\n",
        "\n",
        "    # train the model using torch:\n",
        "    batch_size = 100\n",
        "    epochs = 10\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, X_train.shape[0], batch_size):\n",
        "            x = X_train[i:i + batch_size]\n",
        "            y = Y_train[i:i + batch_size]\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# we use a 3-layer MLP as the discriminative model\n",
        "\n",
        "def get_discriminative_model(input_shape):\n",
        "    \"\"\"\n",
        "    The MLP model for discriminative active learning, without any regularization techniques.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_shape : int\n",
        "        The number of features in the dataset.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : tf.keras.Sequential\n",
        "        The MLP model.\n",
        "    \"\"\"\n",
        "    width = input_shape\n",
        "    model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(width, 100),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(100, 100),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(100, 2),\n",
        "        torch.nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qPMYfBpqBXEQ"
      },
      "source": [
        "**Example Implementation:** compute the H-Divergence between two randomly chosen subsets of CIFAR 10 (should be close to zero)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oowbf9IxBXER"
      },
      "outputs": [],
      "source": [
        "# set up functions for CIFAR 10 example\n",
        "\n",
        "# load the data\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# define the model\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "\n",
        "\n",
        "# define the loss function and the optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "\n",
        "# train the network\n",
        "\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the latent representation of the data using the trained model\n",
        "\n",
        "def get_latent_representation(model, X):\n",
        "    \"\"\"\n",
        "    A function that computes the latent representation of the data using the trained model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: torch.nn.Sequential\n",
        "        The trained model.\n",
        "\n",
        "    X : numpy.ndarray\n",
        "        The data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    latent_representation : numpy.ndarray\n",
        "        The latent representation of the data.\n",
        "    \"\"\"\n",
        "    X = torch.from_numpy(X).float()\n",
        "    latent_representation = model(X)\n",
        "    latent_representation = latent_representation.detach().numpy()\n",
        "\n",
        "    return latent_representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ponm7UjLBXEU",
        "outputId": "1596a2c6-f933-4508-db06-3ba52e36b672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 4ms/step\n",
            "1875/1875 [==============================] - 3s 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "605/605 - 3s - loss: 0.0846 - accuracy: 0.9917 - 3s/epoch - 5ms/step\n",
            "Epoch 2/10\n",
            "605/605 - 1s - loss: 0.0483 - accuracy: 0.9917 - 1s/epoch - 2ms/step\n",
            "Epoch 3/10\n",
            "605/605 - 2s - loss: 0.0481 - accuracy: 0.9917 - 2s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "605/605 - 1s - loss: 0.0481 - accuracy: 0.9917 - 1s/epoch - 2ms/step\n",
            "Epoch 5/10\n",
            "605/605 - 1s - loss: 0.0481 - accuracy: 0.9917 - 1s/epoch - 2ms/step\n",
            "Epoch 6/10\n",
            "605/605 - 1s - loss: 0.0479 - accuracy: 0.9917 - 1s/epoch - 2ms/step\n",
            "Epoch 7/10\n",
            "605/605 - 1s - loss: 0.0480 - accuracy: 0.9917 - 1s/epoch - 2ms/step\n",
            "Epoch 8/10\n",
            "605/605 - 1s - loss: 0.0481 - accuracy: 0.9917 - 1s/epoch - 2ms/step\n",
            "Epoch 9/10\n",
            "605/605 - 1s - loss: 0.0480 - accuracy: 0.9917 - 1s/epoch - 2ms/step\n",
            "Epoch 10/10\n",
            "605/605 - 2s - loss: 0.0480 - accuracy: 0.9917 - 2s/epoch - 3ms/step\n",
            "16/16 [==============================] - 0s 2ms/step\n",
            "1875/1875 [==============================] - 3s 2ms/step\n",
            "0.0005562470753987629\n"
          ]
        }
      ],
      "source": [
        "# compute H-Divergence between two random samples from the train set of size 500\n",
        "\n",
        "X_train = trainset.data\n",
        "\n",
        "# choose 2 x 500 random samples from the training set\n",
        "X_train_1 = X_train[np.random.choice(X_train.shape[0], 500, replace=False)]\n",
        "X_train_2 = X_train[np.random.choice(X_train.shape[0], 500, replace=False)]\n",
        "\n",
        "# convert the samples so that they are in the right format for the model\n",
        "X_train_1 = X_train_1.reshape(X_train_1.shape[0], 3, 32, 32)\n",
        "X_train_2 = X_train_2.reshape(X_train_2.shape[0], 3, 32, 32)\n",
        "\n",
        "# compute the latent representation of the samples\n",
        "\n",
        "latent_representation_train_1 = get_latent_representation(net, X_train_1)\n",
        "latent_representation_train_2 = get_latent_representation(net, X_train_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define a discriminator model with input shape the size of the latent representation\n",
        "# and train it on the latent representation of the samples\n",
        "\n",
        "# get the dimension of the latent representation\n",
        "input_shape = latent_representation_train_1.shape[1]\n",
        "\n",
        "# train the discriminator model\n",
        "discriminator = train_discriminative_model(latent_representation_train_1, latent_representation_train_2, input_shape)\n",
        "\n",
        "# compute the H-Divergence between the latent representations of samples 1 and 2\n",
        "H_divergence = compute_H_divergence(latent_representation_train_1, latent_representation_train_2, discriminator)\n",
        "\n",
        "print(H_divergence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpHbs0tlGbJh"
      },
      "source": [
        "**Interpretation of Results:** The H-Divergence in this case is essentially zero, which is what we expected as the samples were chosen randomly."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5beae9835b3277c3a4a8c87413b972e297eaccb765a3f62b691c35696bfb6223"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
